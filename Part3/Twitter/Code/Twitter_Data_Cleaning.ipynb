{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import pandas as pd\n",
    "with open('tweets-20-04.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "train = pd.read_csv('tweets-20-04.csv',encoding=result['encoding'])\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "data_fr=[]\n",
    "for line in train.iterrows():\n",
    "    line = str(line[1])\n",
    "    data_fr.append(word_tokenize(line[5:len(line)-27]))\n",
    "data_fr_lower=[]\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for line in data_fr:\n",
    "    data_fr_lower.append([w.lower() for w in line])\n",
    "\n",
    "stripped=[]\n",
    "for line in data_fr_lower:\n",
    "    stripped.append([w.translate(table) for w in line])\n",
    "\n",
    "words=[]\n",
    "for line in stripped:\n",
    "    words.append([word for word in line if word.isalpha()])\n",
    "\n",
    "words_data = []\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('rt')\n",
    "stop_words.add('unnamed')\n",
    "for line in words:\n",
    "    words_data.append([w for w in line if not w in stop_words])\n",
    "\n",
    "twitter_strings = []\n",
    "for line in words_data:\n",
    "    string=''\n",
    "    for word in line:\n",
    "        string = string + str(word) + ' '\n",
    "    twitter_strings.append(string)\n",
    "twitter_df = pd.DataFrame(twitter_strings)\n",
    "twitter_df.to_csv(\"twitter_data_small.txt\",header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "import pandas as pd\n",
    "with open('tweets-16-04.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "train = pd.read_csv('tweets-20-04.csv',encoding=result['encoding'])\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "data_fr=[]\n",
    "for line in train.iterrows():\n",
    "    line = str(line[1])\n",
    "    data_fr.append(word_tokenize(line[5:len(line)-27]))\n",
    "data_fr_lower=[]\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for line in data_fr:\n",
    "    data_fr_lower.append([w.lower() for w in line])\n",
    "\n",
    "stripped=[]\n",
    "for line in data_fr_lower:\n",
    "    stripped.append([w.translate(table) for w in line])\n",
    "\n",
    "words=[]\n",
    "for line in stripped:\n",
    "    words.append([word for word in line if word.isalpha()])\n",
    "\n",
    "words_data = []\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('rt')\n",
    "stop_words.add('unnamed')\n",
    "for line in words:\n",
    "    words_data.append([w for w in line if not w in stop_words])\n",
    "\n",
    "twitter_strings = []\n",
    "for line in words_data:\n",
    "    string=''\n",
    "    for word in line:\n",
    "        string = string + str(word) + ' '\n",
    "    twitter_strings.append(string)\n",
    "twitter_df = pd.DataFrame(twitter_strings)\n",
    "twitter_df.to_csv(\"twitter_data.txt\",header=False,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
